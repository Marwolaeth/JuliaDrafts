using TextAnalysis
using WordTokenizers
using Languages
using LinearAlgebra, SparseArrays
import StatsBase: countmap

include("cosine.jl")

text1 = """
МОСКВА, 23 ноября, ФедералПресс. В среду в Москве прошел международный форум «Диалог о фейках», в котором приняли участие представители федеральных органов исполнительной власти, международные эксперты, представители СМИ и участники бизнес-сообщества. Как рассказали организаторы, участники форума обсудили механизмы борьбы с обилием фейков, появляющихся в эпоху информационной войны. Форумчане пришли к выводу, что в подобных сражениях невозможно эффективно отражать информационные атаки противника без создания соответствующей инфраструктуры, основу которой сегодня составляют технологические сервисы.
"""

text2 = """
IT-эксперт Фомин: для выявления шпионских программ на смартфоне нужен антивирус

Найти шпионский софт в мобильном телефоне помогут три способа. О них рассказал эксперт по информационной безопасности Лиги цифровой экономики Виталий Фомин в беседе с «Прайм».

Главный способ — установка антивируса. Специальная программа обнаружит и удалит вредоносное приложение.

Второй метод более долгий — это анализ изменения поведения смартфона. Если батарея телефона стала быстро разряжаться, вырос расход мобильного интернета, произвольно начала включаться геолокация и сообщения сами по себе отправляются на непонятные номера, то это указывает на поселившийся вирус в устройстве.

По словам специалиста, третий способ применяют дополнительно. Это скачивание специального сканера приложений, который способен находить подозрительный доступ. В случае обнаружения вредоносной программы пользователя уведомляют через сообщение на экране.

«Для защиты от шпионского программного обеспечения необходимо скачивать мобильные приложения только из официальных магазинов. Подозрительно, когда приложение требует доступ к функциям, которые ему не нужны», — сказал Фомин.
"""

text3 = """
В Москве состоялся форум «Диалог о фейках», организованный АНО «Диалог регионы». Участники оценили влияние фейков на политическую и экономическую ситуацию в стране и определили степень угроз, исходящих от недостоверных материалов.
"""

text4 = """
Проблему обилия фейков со стороны противников обсудили представители органов власти, фактчекеры и создатели контента на форуме «Диалог о фейках»

МОСКВА, 23 ноября, ФедералПресс. В среду в Москве прошел международный форум «Диалог о фейках», в котором приняли участие представители федеральных органов исполнительной власти, международные эксперты, представители СМИ и участники бизнес-сообщества.

Как рассказали организаторы, участники форума обсудили механизмы борьбы с обилием фейков, появляющихся в эпоху информационной войны. Форумчане пришли к выводу, что в подобных сражениях невозможно эффективно отражать информационные атаки противника без создания соответствующей инфраструктуры, основу которой сегодня составляют технологические сервисы.
"""

text5 = """
В среду 23 ноября в Москве прошел форум «Диалог о фейках» с участием представителей органов власти, международных экспертов, журналистов и бизнес-сообщества. Участники форума обсудили механизмы борьбы с обилием фейков в эпоху информационной войны. Эксперы сошлись на том, что невозможно эффективно отражать информационные атаки противника без создания соответствующей инфраструктуры, основу которой сегодня составляют отечественные технологические сервисы.

Также участники оценили влияние фейков на политическую и экономическую ситуацию в стране и определили степень угроз, исходящих от недостоверных материалов.
"""

NGramDocument(text1, 2)

# Tokenize the text
tokens = punctuation_space_tokenize.(lowercase.([text1, text2, text3, text4, text5]))

# Remove stopwords
stops = stopwords(Languages.Russian())
stops_new = readlines("data/stopwords_ru.txt")
stops = union(stops, stops_new)
"который" ∈ stops
filtered_tokens = filter.(word -> !(word ∈ stops), tokens)

# hz = TextAnalysis.ngramizenew(filtered_tokens[1], 1, 2)

# countmap(hz)
# TextAnalysis.ngramize(Languages.Russian(), filtered_tokens[1], 2)

# The old way
docs = [NGramDocument(TextAnalysis.ngramize(Languages.Russian(), tokens, 2)) for tokens ∈ filtered_tokens]

# The new way
docs = [
    NGramDocument(countmap(String.(TextAnalysis.ngramizenew(tokens, 1, 2))))
    for tokens ∈ filtered_tokens
]

fieldnames(NGramDocument)

crps = Corpus(docs)
update_lexicon!(crps)
crps
languages!(crps, Languages.Russian())
TextAnalysis.stemmer_for_document(crps[1])
# stem!(crps)
# fieldnames(Corpus)
crps.lexicon
m = DocumentTermMatrix(crps)
dtm(m)
Cosine.cosine(dtm(m))
# Cool!